%!TEX root = thesis.tex

\chapter{Background}

\section{Brief History of Social Choice Theory}

	Election systems are not a recent invention. The earliest democracies resembling what we know today date back to around 508 BC in Athens, Greece. The general idea of elections was used even before that in many other parts of the world \cite{democracybritannica}. In Athens, the assembly was the core of democracy, and any male citizen of at least eighteen years of age was allowed to attend, and therefore, to vote \cite{heinemann1952}. Athenians voted directly on public policy, instead of electing representatives, and voting was done by majority rule. Outside of the assembly, a process known as ostracism was used to exile individuals if necessary. This was done using the plurality voting rule, whereby each man wrote a name on a piece of pottery and the person with the most votes was exiled \cite{oturnbull}.

	Both the majority rule and the plurality systems used in early Greek democracy were very simple. One drawback of these systems is that each voter could only voice a preference for a single candidate. A more accurate way to represent each voter's opinion is with a ranked list of all candidates. This way if candidates tie for first place, the tie can easily be broken by looking at the voter's second choice. If there is still a tie, then the third choice can be taken into account, and so forth. This ranked list is called a preference list.

	In 1770 Jean-Charles de Borda proposed an election system, known now as the Borda count, as a way of electing members of the French Academy of Sciences \cite{borda1781mémoire}. In the Borda count system, each candidate receives points based on their rank in each voter's preference list, i.e. for each first place ranking a candidate will get the most points, for each second place ranking a candidate will get slightly less points, and so on. The winning candidate is the one who receives the greatest total number of points. It was around the time Borda proposed this system that election systems began to be studied academically, though recently it has been discovered that Ramon Llull came up with the Borda count even earlier, in the 13th century \cite{hägele2001llull}.

	Majority rule, plurality, and the Borda count are a few examples of voting rules, but there are many others. Given the large number of voting rules, and that each rule seems to have various strengths and weaknesses, it is useful to compare them to each other. The most obvious criteria for a good election system is fairness \cite{chevaleyre2006issues}. It seems natural that the election system which best represents the constituents' preferences is the best system. Fairness of an election system is easy to recognize if there are only two candidates: the candidate who is preferred by the majority of voters should win. But with a larger number of candidates determining the fairness of an election system is not so obvious.

	Interest in the fairness of voting systems prompted Marquis de Condorcet, a contemporary of Borda, to propose that the winning candidate of an election be the candidate who would win a head-to-head election against each of the other candidates (1785). Such a winner is known as the \emph{Condorcet winner}. Unfortunately, Condorcet also proved that a Condorcet winner does not always exist because majority preferences are intransitive in elections with more than two alternatives \cite{le1785essai, black1998theory}. In other words, it is possible to have alternative $a \succ $(beats)$ b$, $b \succ c$, $c \succ a$. A voting rule that gives the Condorcet winner if one exists is said to satisfy the \emph{Condorcet criterion}. The Condorcet criterion was one of the first formal fairness criteria, and is still widely used today.

	In 1876, Charles Dodgson (also known as Lewis Carroll) proposed an election system satisfying the Condorcet criterion known as Dodgson's method. Dodgson's method declares the winner to be whichever alternative can become a Condorcet winner with the fewest adjacent swaps in voters' preference lists \cite{dodgson1876method}. More precisely, given the original profile $p$, we select a profile $p'$ such that $p'$ has a Condorcet winner and the total Kendall tau distance (see Definition \ref{kendall-tau-definition}) between $p$ and $p'$ is minimum (compared to all possible profiles). Then the winner is the alternative that wins under $p'$. One major drawback of this method is that computing the winner is NP-hard \cite{bartholdi1989voting}.

	In 1950, Kenneth Arrow, an American economist who was interested in the fairness of social welfare functions, made a large contribution to the field of Social Choice Theory with his impossibility theorem. Arrow's theorem \cite{arrow1950difficulty} (which he strengthened in 1963 \cite{arrow1963social}) demonstrates that no social welfare function can ``fairly'' convert the preferences of voters into a society-wide preference list. While ``fair'' is clearly subjective, he gave a list of basic properties which seem intuitively required for fairness:
	\begin{description}
		\item[Unrestricted domain (universality)] All individual preferences are allowed, and yield a valid group preference.
		\item[Independence of irrelevant alternatives] If all voters' preferences between alternatives $x$ and $y$ remain the same, the group preference between $x$ and $y$ is unchanged even if voters change their preferences regarding other alternatives.
		\item[Pareto principle (unanimity)] Unanimity of individual preferences implies a group preference. E.g. if all individuals prefer alternative $x$ to $y$, then the group will prefer $x$ to $y$.
		\item[Non-dictatorship] There is no voter whose preference always dictates the group preference.
	\end{description}
	Arrow proved that these properties are inconsistent: no voting system can satisfy all of these properties, hence, no social welfare function can be completely fair.

	The work done by Condorcet and Arrow is widely regarded as being foundational to the modern field of Social Choice Theory, and marks a transition from viewing social choice as a purely practical problem to a more rigorous theoretical study.


\section{History of Manipulation}

	One problem relating to the issue of fairness in social choice is that of manipulation (or strategic voting or tactical voting). Manipulation is when an individual purposefully misrepresents his preferences hoping to get a more favorable outcome in the election. For example, if a voter knows that his most preferred alternative has no chance of winning the election, he may instead say that he prefers a different alternative, so that even though his favorite alternative cannot win, at least his second choice alternative has a better chance of winning. This manipulation will benefit the voter but will not benefit society in general, because by lying about his preferences the voter has skewed the results of the election in his favor. Therefore, it is beneficial to search for ways to avoid manipulation in social choice.

	One way to avoid manipulation would be to devise a voting rule that is non-manipulable. Unfortunately, in 1973 the Gibbard-Satterthwaite theorem was published which states that every voting rule satisfying the following properties is subject to manipulation.
	\begin{description}
		\item[Non-dictatorship] (Defined above, in Arrow's theorem)
		\item[Non-imposition] Every alternative has the possibility of winning.
	\end{description}
	It would certainly seem that any reasonable voting rule would need to satisfy both of these criteria, hence, any reasonable voting rule is manipulable \cite{gibbard1973manipulation, satterthwaite1975strategy, duggan2000strategic}. This means that we cannot make manipulation impossible via a cleverly devised voting rule --- a rather disappointing prospect.

	Until this point in the history, social choice theory had been separate from computer science --- and indeed computer science was a very young discipline at this point. A new sub-field of social choice theory was spawned, computational social choice theory, which applies the studies of computer science to social choice theory. In 1989, Bartholdi, Tovey, and Trick proposed a computational barrier to manipulation in election systems \cite{bartholdi1989computational}. Instead of trying to make manipulation impossible, they endeavored to make it computationally intractable \cite{chevaleyre2007short}; even if a profitable manipulation exists it is of no use in practice if it is computationally infeasible to find. They were able to demonstrate that while many voting rules are easy to manipulate (a manipulation can be found in polynomial time), the problem of finding a manipulation for certain scoring rules is NP-complete. They called rules that can be manipulated in polynomial time \emph{vulnerable}, and those for which manipulation is NP-hard \emph{resistant}. For a formal definition of manipulation, see Definition \ref{manipulation-definition}.

	This ushered in a new way to approach social choice: from a computational footing. Bartholdi, Tovey, and Trick also studied the computational difficulty of finding a winner for various voting rules. For example, they showed that the Dodgson method mentioned above \cite{dodgson1876method} is actually infeasible to manipulate for the simple reason that figuring out the winner of the election is NP-hard. Therefore, it is not sufficient for a desirable voting rule to be hard to manipulate; it must also be also be efficient to determine a winner.

	In 1991, Bartholdi and Orlin \cite{bartholdi1991single} added to the above results by showing that the Single Transferable Vote (STV) rule was both resistant to manipulation, and quick to determine a winner. Although STV has problems of its own \cite{brams1982ams, doron1977single, fishburn1983paradoxes, holzman1989vote, moulin1988condorcet}, it is encouraging to see that it is possible for an efficient voting rule to resist manipulation.

	In 2002, Conitzer and Sandholm took a slightly different approach \cite{conitzer2002vote} (which they later extended \cite{conitzer2007elections}), studying coalition manipulation. Instead of a single voter manipulating an election, a group (coalition) of voters work together to manipulate an election. This vein of research has since been extended in various directions \cite{conitzer2003universal, elkind2005hybrid, faliszewski2006complexity, hemaspaandra2007anyone, procaccia2007multi, elkind2005small}.

	The work mentioned so far which attempts to erect a computational barrier to manipulation is encouraging, and may indeed provide ways to prevent manipulation in election systems. However it deals with the worst-case complexity of manipulation. In 2006, work by Conitzer and Sandholm \cite{conitzer2006nonexistence} along with that of Procaccia and Rosenschein \cite{procaccia2006junta} showed that while manipulation can be hard in the worst case, it is much easier in the average case. In the next few years more work was done to make this concern even more well-founded \cite{procaccia2007average, erdelyi2007approximating}. Work along these lines by Friedgut, Kalai, and Nisan \cite{friedgut2008elections} in 2008 is the main inspiration for this thesis, and has also spawned other work which will be discussed further in the Related Work chapter.
