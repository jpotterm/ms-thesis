%!TEX root = thesis.tex


\chapter{Introduction}

	In this thesis we endeavor to extend the results of Friedgut, Kalai, and Nisan \cite{friedgut2008elections} who proved that social choice functions can be successfully manipulated by random preference reordering with non-negligible probability. However, there are two main restrictions on their results: the social choice function must be neutral, and the election must have at most 3 alternatives. We attempt to remove the later restriction in order to generalize the results to elections with any number of candidates.

	Our proof uses many aspects of Friedgut, Kalai, and Nisan's proof. Their proof is done in three steps, with the first two steps being already written in general terms, while the third is restricted to 3 alternatives. Therefore we need only generalize the third step. Our methods rely heavily on lattice theory and combinatorics.


\section{Importance}

	The statement that election systems are important to society needs no defense, as they are essential to democracy which undergirds the governments of many nations. Furthermore, election systems are used in many non-political situations --- anywhere a group of independent agents needs to come to a consensus. This could be a school or a business electing board members, or stock holders voting on an issue affecting the company. Indeed, election systems are not even wholly reserved to humans. Elections can be used by artificial intelligence systems when a group of agents needs to make a decision \cite{ephrati1991clarke, ephrati1993multi, pennock2000social, dwork2001rank, fagin2003efficient}, or they can be used in internet page ranking algorithms for search engines \cite{chevaleyre2007short}.

	However it is less clear that manipulation in elections --- especially random manipulation --- is important. Therefore, we describe its importance by briefly explaining the background leading up to it.

	One obvious criteria for a good election system is fairness \cite{chevaleyre2006issues}, because we would like the winning candidate to be the one which best represents the constituents' preferences. Fairness of an election system is easy to recognize if there are only two candidates: the candidate who is preferred by the majority of voters should win. But with a larger number of candidates, determining the fairness of an election system is not so obvious.

	Marquis de Condorcet, was one of the first people to study (academically) the issue of fairness in election systems. He proposed that the winning candidate be the candidate who would win a head-to-head election against each of the other candidates, and such a winner is known as the \emph{Condorcet winner}. Unfortunately, Condorcet also proved that a Condorcet winner does not always exist. Nevertheless, the Condorcet criterion was one of the first formal fairness criteria, and is still widely used today.

	In 1950, Kenneth Arrow, an American economist who was interested in the fairness of social welfare functions, made a large contribution to the field of Social Choice Theory with his impossibility theorem \cite{arrow1950difficulty, arrow1963social}. This theorem demonstrates that no social welfare function can ``fairly'' convert the preferences of voters into a society-wide preference list by showing that no social welfare function can satisfy the following criteria (which will be further described in the next chapter): unrestricted domain, independence of irrelevant alternatives, unanimity, and non-dictatorship.

	One of the great enemies of fairness in election systems is mainpulation (or strategic voting or tactical voting). Manipulation is when an individual purposefully misrepresents his preferences hoping to get a more favorable outcome in the election. One way to avoid manipulation would be to devise a voting rule that is non-manipulable. Unfortunately, the Gibbard-Satterthwaite theorem states that every voting rule which is not a dictatorship and under which any alternative can win is subject to manipulation \cite{gibbard1973manipulation, satterthwaite1975strategy, duggan2000strategic}. This means that we cannot make manipulation impossible via a cleverly devised voting rule.

	In an attempt to circumvent the Gibbard-Satterthwaite, Bartholdi, Tovey, and Trick studied the computational difficulty of finding a winner for various voting rules. For example, they showed that the Dodgson method mentioned above \cite{dodgson1876method} is actually infeasible to manipulate for the simple reason that figuring out the winner of the election is NP-hard. Therefore, it is not sufficient for a desirable voting rule to be hard to manipulate; it must also be also be efficient to determine a winner.

	Many others have followed in the vein of searching for a computational barrier to manipulation, but the majority of computational results deals with the worst-case complexity of manipulation. In 2006, work by Conitzer and Sandholm \cite{conitzer2006nonexistence} along with that of Procaccia and Rosenschein \cite{procaccia2006junta} showed that while manipulation can be hard in the worst case, it is much easier in the average case. In the next few years more work was done to make this concern even more well-founded \cite{procaccia2007average, erdelyi2007approximating}.

	Friedgut, Kalai, and Nisan \cite{friedgut2008elections} in 2008, instead of studying worst-case manipulation, performed a probabilistic analysis of random manipulation. That is, instead of a voter intelligently manipulating an election, which can be difficult in terms of worst-case complexity, he simply chooses his manipulation randomly (if his most preferred candidate is not winning already). They proved that even a random manipulation will succeed with non-negligible probability. This is significant because no matter how hard it is in the worst-case to find a profitable manipulation, if it is trivial to find a random manipulation, that could be enough.


\section{Difficulty}

	The difficulty of this problem can be seen by the recent work done in generalizing the results of Friedgut, Kalai, and Nisan. Firstly, its difficulty can be seen by Friedgut, Kalai, and Nisan themselves failing to generalize it, both in the original paper \cite{friedgut2008elections}, and also later when they removed the neutrality constraint \cite{friedgut2011quantitative}. If it were an easy task, they would have done it from the outset.

	In addition, other authors have done work along the same lines, but still without coming up with a general result. In 2008 Xia and Conitzer were able to prove a similar theorem for any number of candidates, but instead of neutrality they assumed 5 other conditions for the voting rule \cite{xia2008sufficient}:

	\begin{description}
		\item[Homogeneity] For any $n \in \mathbb{N}$ we have:
			\begin{align*}
				f(P) = f\left(\bigcup_{i=1}^n P\right).
			\end{align*}
		\item[Anonymity] The result of the election does not depend on the names of the voters. Formally, given a profile $P$ and a permutation $\sigma(P)$: $f(P) = f(\sigma(P))$.
		\item[Non-imposition] (Defined above, in the Gibbard-Satterthwaite theorem)
		\item[Canceling out] Adding the set of all linear orders to the votes does not change the result. More formally, for any profile $P$ we have that: $f(P) = f(P \cup L(C))$.
		\item[Stability] Given alternatives $C = \{c_1, c_2, \ldots, c_m\}$, there exists a profile $P$ such that:
			\begin{enumerate}
				\item $P$ and $D_{m}(P)$ are both stable (slight modifications don't change the winner)
				\item $f(P) = c_1$
				\item $f(D_{m}(P)) = c_2$
			\end{enumerate}
			Where $D_m$ is defined such that if $D_m(P) = P'$, then $P|_{C \backslash c_m} = P'|_{C \backslash c_m}$ and the position of $c_m$ is uniformly distributed in $P'$. For a formal definition of $D_m$ and of ``stability'', see the original paper \cite{xia2008sufficient}.
	\end{description}

	However, these conditions are stricter than the neutrality assumption of Friedgut, Kalai, and Nisan, in the sense that they do not capture all of the ``common'' voting rules, e.g. Bucklin.

	Around the same time Dobzinski and Procaccia published complementary results for two voters and social choice functions satisfying unanimity (the Pareto principle) \cite{dobzinski2008frequent}. They proved the following:

	\begin{theorem}[Dobzinski and Procaccia]
		Let $f$ be a Pareto-optimal SCF and let $n = 2$, $m \ge 3$, and $\delta < \frac{1}{32m^9}$. If $f$ is $\delta$-strategyproof then $f$ is $16m^8 \delta$-dictatorial.
	\end{theorem}

	The fact that all of these authors worked on the same problem over multiple years and were unable to achieve a general result speaks to the difficulty of this problem.


\section{Independent Work}

	% This should go a bit later, but I'm not sure where

	Unfortunately for us, but fortunately for the field of social choice theory as a whole, Isaksson, Kindler, and Mossel \emph{have}, independently during the writing of this thesis, published a brilliant generalization of the original theorem of Friedgut, Kalai, and Nisan and even improved slightly upon the results \cite{isaksson2010geometry}. Translating their results into the terminology we have been using, they proved that for a neutral social choice function $f$ with $m \ge 4$ alternatives and $n$ voters that is $\epsilon$-far from dictatorship, a uniformly chosen profile will be manipulable with probability at least $2^{-1} \epsilon^2 n^{-4} m^{-6} (m!)^{-3}$.

	Later Friedgut et al. removed the neutrality constraint from their original theorem, and added an author \cite{friedgut2011quantitative}.

	Finally, Mossel and R\'{a}cz \cite{mossel2011quantitative} took ideas from these two proofs and created a unified proof with the same results as Isaksson, Kindler, and Mossel, but without the neutrality constraint.

	Though these results have independently achieved the goals we set out with, we believe that our work is still useful. At the very least ours simply stands as an alternate proof. However, our proof has the benefit that it uses very similar techniques to those of the original proof of Friedgut, Kalai, and Nisan. Additionally we believe that our proof is much simpler and easier to understand.


\section {Proof Summary}

	The proof we are generalizing is broken up into three steps. In the original paper they are called Step 1, Step 2, and Step 3, but in \cite{friedgut2011quantitative} they are called the following respectively:
	\begin{enumerate}
		\item Applying a quantitative version of Arrow's impossibility theorem
		\item Reduction from low manipulation power to low dependence on irrelevant alternatives
		\item Reduction from low manipulation power to low dependence on irrelevant alternatives
	\end{enumerate}
	In this thesis we will refer to them as Step 1, Step 2, and Step 3.

	In the original paper, Friedgut, Kalai, and Nisan were able to generalize Step 1 and Step 2 as follows:

	\begin{lemma}[Generalized Step 1]
		For every fixed $m$ and $\epsilon > 0$ there exists $\delta > 0$ such that if $F = f^{\otimes \binom{m}{2}}$ is a neutral IIA GSWF over $m$ alternatives with $f : \{0,1\}^n \rightarrow \{0,1\}$, and $\Delta(f, DICT) > \epsilon$, then $F$ has probability of at least $\delta \ge (C\epsilon)^{\lfloor m/3 \rfloor}$ of not having a Generalized Condorcet Winner, where $C > 0$ is an absolute constant.
	\end{lemma}

	\begin{lemma}[Generalized Step 2]
		For every fixed $m$ there exists $\delta > 0$ such that for all $\epsilon > 0$ the following holds. Let $f$ be a neutral SCF among $m$ alternatives such that $\Delta(f, DICT) > \epsilon$. Then for all $(a,b)$ we have $M^{a,b}(f) \ge \delta$.
	\end{lemma}

	Therefore we focus on generalizing Step 3. The original Step 3 was:

	\begin{lemma}[Non-General Step 3]
		For every SCF $f$ on $3$ alternatives and every $a,b \in A$, $M^{a,b} \le \sum_i M_i \cdot 6$
	\end{lemma}

	And our generalization is:

	\begin{lemma}[Generalized Step 3]
		For every SCF $f$ on $m$ alternatives and every $a,b \in A$, $M^{a,b} \le \sum_i M_i \cdot m!$
	\end{lemma}

	When we put together all 3 generalized steps we get our main result:
	\begin{theorem}[Main Result]
		There exists a constant $C > 0$ such that for every $\epsilon > 0$ the following holds. If $f$ is a neutral SCF for $n$ voters over 3 alternatives and $\Delta(f, g) > \epsilon$ for any dictatorship $g$, then $f$ has total manipulatiblity: $\sum^n_{i=1} M_i(f) \ge \frac{(C\epsilon)^{\lfloor m/3 \rfloor}}{m!}$.
	\end{theorem}


\section{Structure of the Remaining Chapters}

	\begin{description}
		\item[Chapter 2: Preliminaries] In the next chapter we introduce the preliminaries. These include formal definitions and notation to serve as a reference for use in the rest of the thesis. The preliminaries are often elementary but provide a technical foundation for the following work.

		\item[Chapter 3: Background] Here we give some background information on the field of social choice theory and describe how it has evolved eventually to lead to the problem we are solving.

		\item[Chapter 4: Related Work] In the related work chapter we will describe in a moderate amount of detail the results and methods of various other authors relating to the work of Friedgut, Kalai, and Nisan and, hence, to ours.

		\item[Chapter 5: Results] This is the technical portion of the thesis in which we prove some foundational lemmas and eventually build up a proof of our main results.
	\end{description}
