%!TEX root = thesis.tex

\chapter{Introduction}

	% Though election systems have traditionally been used in politics, they have also been applied to many other situations. Election systems are used in schools to elect board members, and they are used in businesses for stock holders to vote on the direction of a company. Recently, artificial intelligence systems have been holding elections when a group of intelligent agents need to reach a common consensus \cite{ephrati1991clarke, ephrati1993multi, pennock2000social, dwork2001rank, fagin2003efficient}, and in internet page ranking algorithms for search engines \cite{chevaleyre2007short}.

	% Because election systems are so widespread and are crucial for the functioning of society, it is beneficial to study their nature along with their strengths and weaknesses. Election systems have been studied academically since as early as the 13th century \cite{hägele2001llull}, but more recently they have been scrutinized by the computer science community in the fields of theory and artificial intelligence.


% \section{Brief History of Social Choice Theory}

	Election systems resembling what we know today date back to around 508 BC in Athens, Greece. The greeks used both the majority rule and the plurality systems which are very simple systems with one disadvantage being that each voter can only voice a preference for a single candidate. A more accurate way to represent each voter's opinion is with a ranked list of all candidates so, for instance, if candidates tie for first place, the tie can easily be broken by looking at voters' second choice, and then the third choice can be taken into account, etc. We call this ranked list a preference list.

	There weren't many improved ideas for election systems until much later, in the 13th century when Ramon Llull studied election systems \cite{hägele2001llull}. In 1770 Jean-Charles de Borda independently proposed an election system similar to Llull's, now called the Borda count, as a way of electing members of the French Academy of Sciences \cite{borda1781mémoire}. In the Borda count system, each candidate receives points based on their rank in each voter's preference list, with the winner being the candidate to receive the most points. Majority rule, plurality, and the Borda count are a some examples of voting rules, but there are many others, each with various strengths and weaknesses. Therefore, it is useful to compare them to each other. The most obvious criteria for a good election system is fairness \cite{chevaleyre2006issues}, because we would like the winning candidate to be the one which best represents the constituents' preferences. Fairness of an election system is easy to recognize if there are only two candidates: the candidate who is preferred by the majority of voters should win. But with a larger number of candidates, determining the fairness of an election system is not so obvious.

	Marquis de Condorcet, a contemporary of Borda, was interested in the fairness of voting systems and he proposed that the winning candidate be the candidate who would win a head-to-head election against each of the other candidates. Such a winner is known as the \emph{Condorcet winner}. Unfortunately, Condorcet also proved that a Condorcet winner does not always exist. Nevertheless, the Condorcet criterion was one of the first formal fairness criteria, and is still widely used today.

	In 1950, Kenneth Arrow, an American economist who was interested in the fairness of social welfare functions, made a large contribution to the field of Social Choice Theory with his impossibility theorem \cite{arrow1950difficulty, arrow1963social}. This theorem demonstrates that no social welfare function can ``fairly'' convert the preferences of voters into a society-wide preference list by showing that no social welfare function can satisfy the following criteria (which will be further described in the next chapter): unrestricted domain, independence of irrelevant alternatives, unanimity, and non-dictatorship.

	The work done by Condorcet and Arrow is widely regarded as being foundational to the modern field of Social Choice Theory, and marks a transition from viewing social choice as a purely practical problem to a more rigorous theoretical study.

	The main problem we will be dealing with relating to the issue of fairness in social choice is that of manipulation (or strategic voting or tactical voting). Manipulation is when an individual purposefully misrepresents his preferences hoping to get a more favorable outcome in the election. One way to avoid manipulation would be to devise a voting rule that is non-manipulable. Unfortunately, the Gibbard-Satterthwaite theorem states that every voting rule which is not a dictatorship and under which any alternative can win is subject to manipulation \cite{gibbard1973manipulation, satterthwaite1975strategy, duggan2000strategic}. This means that we cannot make manipulation impossible via a cleverly devised voting rule.

	In an attempt to circumvent the Gibbard-Satterthwaite, Bartholdi, Tovey, and Trick studied the computational difficulty of finding a winner for various voting rules. For example, they showed that the Dodgson method mentioned above \cite{dodgson1876method} is actually infeasible to manipulate for the simple reason that figuring out the winner of the election is NP-hard. Therefore, it is not sufficient for a desirable voting rule to be hard to manipulate; it must also be also be efficient to determine a winner.

	Many others have followed in the vein of searching for a computational barrier to manipulation, but the majority of computational results deals with the worst-case complexity of manipulation. In 2006, work by Conitzer and Sandholm \cite{conitzer2006nonexistence} along with that of Procaccia and Rosenschein \cite{procaccia2006junta} showed that while manipulation can be hard in the worst case, it is much easier in the average case. In the next few years more work was done to make this concern even more well-founded \cite{procaccia2007average, erdelyi2007approximating}.

	Work along these lines by Friedgut, Kalai, and Nisan \cite{friedgut2008elections} in 2008 is the main inspiration for this thesis. Instead of studying worst-case manipulation, they performed a probabilistic analysis of random manipulation. That is, instead of a voter intelligently manipulating an election, which can be difficult in terms of worst-case complexity, he simply chooses his manipulation randomly (if his most preferred candidate is not winning already). They proved that even a random manipulation will succeed with non-negligible probability. This is significant because no matter how hard it is in the worst-case to find a profitable manipulation, if it is trivial to find a random manipulation, that could be enough.

	More formally they defined a metric, \emph{manipulation power} $M_i(f)$, of voter $i$ on a social choice function $f$ to be the probability that $p_i'$ is a profitable manipulation by voter $i$, where $p$ is a profile and $p_i'$ is a preference list which are both chosen uniformly at random. Their main result is that there exists a constant $C$ such that for 3 alternatives, $n$ voters, and a neutral social choice function $f$ which is $\epsilon$-far from dictatorship ($\epsilon > 0$) then

	\begin{align*}
		\sum_{i=1}^n M_i(f) \ge C \epsilon^2
	\end{align*}

	This means that when $\epsilon$ is fixed --- it is once a voting rule is determined --- then some voter has more than his share (a non-negligible amount) of manipulation power: $\max_i M_i(f) \ge \Omega(\frac{1}{n})$ \cite{friedgut2008elections}.

	This result is incredibly powerful, but unfortunately is limited to social choice functions over 3 alternatives. The only assumptions are the \emph{impartial culture} assumption, that votes are selected uniformly at random, and the neutrality of the social choice function.

	The impartial culture assumption is widely used and though many argue that it is not realistic, it is very hard to determine a distribution that \emph{would} be realistic. In this thesis we set out to remove the restriction to 3 alternatives, generalizing this result to any number of alternatives. Additionally, it would have been useful to remove the neutrality constraint, but that was not the main goal of our work.

	Unfortunately for us, but fortunately for the field of social choice as a whole, Isaksson, Kindler, and Mossel have, independently during the writing of this thesis, published a brilliant generalization of the original theorem of Friedgut, Kalai, and Nisan and even improved slightly upon the results \cite{isaksson2010geometry}. Translating their results into the terminology we have been using, they proved that for a neutral social choice function $f$ with $m \ge 4$ alternatives and $n$ voters that is $\epsilon$-far from dictatorship, a uniformly chosen profile will be manipulable with probability at least $2^{-1} \epsilon^2 n^{-4} m^{-6} (m!)^{-3}$.

	Later Friedgut et al. removed the neutrality constraint from their original theorem, and added an author \cite{friedgut2011quantitative}.

	Finally, Mossel and R\'{a}cz \cite{mossel2011quantitative} took ideas from these two proofs and created a unified proof with the same results as Isaksson, Kindler, and Mossel, but without the neutrality constraint.

	Though these results have independently achieved the goals we set out with, we believe that our work is still useful. At the very least ours simply stands as an alternate proof. However, our proof has the benefit that it uses very similar techniques to those of the original proof of Friedgut, Kalai, and Nisan. Additionally we believe that our proof is much simpler and easier to understand.

	%  Overview of hypotheses?

\section {Structure of the Proof}

	The proof we are generalizing is broken up into three steps. In the original paper they are called Step 1, Step 2, and Step 3, but in \cite{friedgut2011quantitative} they are called the following respectively:
	\begin{enumerate}
		\item Applying a quantitative version of Arrow's impossibility theorem
		\item Reduction from low manipulation power to low dependence on irrelevant alternatives
		\item Reduction from low manipulation power to low dependence on irrelevant alternatives
	\end{enumerate}
	In this thesis we will refer to them as Step 1, Step 2, and Step 3.

	In the original paper, Friedgut, Kalai, and Nisan were able to generalize Step 1 and Step 2 as follows:

	\begin{lemma}[Generalized Step 1]
		For every fixed $m$ and $\epsilon > 0$ there exists $\delta > 0$ such that if $F = f^{\otimes \binom{m}{2}}$ is a neutral IIA GSWF over $m$ alternatives with $f : \{0,1\}^n \rightarrow \{0,1\}$, and $\Delta(f, DICT) > \epsilon$, then $F$ has probability of at least $\delta \ge (C\epsilon)^{\lfloor m/3 \rfloor}$ of not having a Generalized Condorcet Winner, where $C > 0$ is an absolute constant.
	\end{lemma}

	\begin{lemma}[Generalized Step 2]
		For every fixed $m$ there exists $\delta > 0$ such that for all $\epsilon > 0$ the following holds. Let $f$ be a neutral SCF among $m$ alternatives such that $\Delta(f, DICT) > \epsilon$. Then for all $(a,b)$ we have $M^{a,b}(f) \ge \delta$.
	\end{lemma}

	Therefore we focus on generalizing Step 3. The original Step 3 was:

	\begin{lemma}[Non-General Step 3]
		For every SCF $f$ on $3$ alternatives and every $a,b \in A$, $M^{a,b} \le \sum_i M_i \cdot 6$
	\end{lemma}

	And our generalization is:

	\begin{lemma}[Generalized Step 3]
		For every SCF $f$ on $m$ alternatives and every $a,b \in A$, $M^{a,b} \le \sum_i M_i \cdot m!$
	\end{lemma}

	When we put together all 3 generalized steps we get our main result:
	\begin{theorem}[Main Result]
		There exists a constant $C > 0$ such that for every $\epsilon > 0$ the following holds. If $f$ is a neutral SCF for $n$ voters over 3 alternatives and $\Delta(f, g) > \epsilon$ for any dictatorship $g$, then $f$ has total manipulatiblity: $\sum^n_{i=1} M_i(f) \ge \frac{(C\epsilon)^{\lfloor m/3 \rfloor}}{m!}$.
	\end{theorem}
