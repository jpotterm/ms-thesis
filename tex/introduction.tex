%!TEX root = thesis.tex


\chapter{Introduction}

	In this thesis we endeavor to extend the results of Friedgut, Kalai, and Nisan \cite{friedgut2008elections} who proved that social choice functions can be successfully manipulated by random preference reordering with non-negligible probability. However, there are two main restrictions on their results: the social choice function must be neutral and the election must have at most 3 alternatives. We attempt to remove the later restriction in order to generalize the results to elections with any number of candidates.

	Our proof draws upon many aspects of Friedgut, Kalai, and Nisan's proof. Their proof is done in three steps, with the first two steps being already written in general terms, while the third is restricted to 3 alternatives. Therefore we need only generalize the third step. We rely heavily on lattice theory and combinatorics to prove this generalization.


\section{Importance}

	It is obvious, and widely accepted that election systems are important to society. They are essential to democracy, which is the foundation of many nations' governments, and are also used in many non-political situations---anywhere a group of independent agents needs to come to a consensus. They are used in schools, electing board members for a business, and stock holders voting on issues affecting a company. Election systems are not even wholly restricted to humans. Elections can be used by artificial intelligence systems when a group of agents needs to make a decision \cite{ephrati1991clarke, ephrati1993multi, pennock2000social, dwork2001rank, fagin2003efficient}, and they can be used in internet page ranking algorithms for search engines \cite{chevaleyre2007short}. Studying the stability of election systems under noise can even be used to determine how resistant certain systems, e.g. network transfers, are to error \cite{shockey2008comparative}.

	However, it is less clear that manipulation in elections---especially random manipulation---is important, so we will attempt to describe its importance by briefly explaining the history behind its investigation.

	One obvious criteria for a good election system is fairness \cite{chevaleyre2006issues}, and it is generally accepted that the winning candidate should, on the whole, represent the will the constituents'. It is easy to recognize a fair election system if there are only two candidates: the one preferred by the majority of voters should win. But with a larger number of candidates, determining the fairness of an election system is more difficult.

	Marquis de Condorcet was one of the first people to study the issue of fairness in election systems. He proposed that the winning candidate be the candidate who would win a head-to-head election against each of the other candidates, and such a winner is known as the \emph{Condorcet winner}. Unfortunately, Condorcet also proved that a Condorcet winner does not always exist. Nevertheless, this criterion for fairness, called the \emph{Condorcet criterion}, was one of the first formal fairness criteria, and is still widely used today.

	In 1950, Kenneth Arrow, an American economist who was interested in the fairness of social welfare functions, made a large contribution to the field of social choice theory with his impossibility theorem \cite{arrow1950difficulty, arrow1963social}. This theorem demonstrates that no social welfare function can ``fairly'' convert the preferences of voters into a society-wide preference list, by showing that no social welfare function can satisfy the following criteria (which will be further described in the next chapter): unrestricted domain, independence of irrelevant alternatives, unanimity, and non-dictatorship.

	One of the great enemies of fairness in election systems is \emph{manipulation} (or strategic voting or tactical voting). Manipulation is when an individual purposefully misrepresents his preferences, hoping to achieve a more favorable outcome in the election. One way to avoid manipulation would be to devise a voting rule that is non-manipulable. Unfortunately, the Gibbard-Satterthwaite theorem states that every voting rule that is not a dictatorship, and under which any alternative can win, is subject to manipulation \cite{gibbard1973manipulation, satterthwaite1975strategy, duggan2000strategic}. This means that we cannot make manipulation impossible via a cleverly devised voting rule.

	In an attempt to circumvent the Gibbard-Satterthwaite theorem, Bartholdi, Tovey, and Trick studied the computational difficulty of finding a winner for various voting rules. For example, they showed that the Dodgson method mentioned above \cite{dodgson1876method} is actually infeasible to manipulate for the simple reason that figuring out the winner of the election is NP-hard. Therefore, it is not sufficient for a desirable voting rule to be hard to manipulate: it must also be also be efficient to determine a winner.

	Many others have followed in the vein of searching for a computational barrier to manipulation, but the majority of these results deal with the worst-case complexity of manipulation. In 2006, work by Conitzer and Sandholm \cite{conitzer2006nonexistence} along with that of Procaccia and Rosenschein \cite{procaccia2006junta} showed that while manipulation can be hard in the worst case, it is often much easier in the average case. In the next few years more work was done to make this concern even more well-founded \cite{procaccia2007average, erdelyi2007approximating}.

	In 2008, Friedgut, Kalai, and Nisan \cite{friedgut2008elections}, instead of studying worst-case manipulation, performed a probabilistic analysis of random manipulation. That is, instead of a voter intelligently manipulating an election, which can be difficult in terms of worst-case complexity, he simply chooses his manipulation randomly (if his most preferred candidate is not already winning). They proved that even a random manipulation will succeed with non-negligible probability. This is significant because no matter how hard it is to find a profitable manipulation in the worst-case, it is trivial to find a random manipulation. If the probability of success of a random manipulation is high enough, it could completely bypass the computational barrier to manipulation. These are the results we hope to extend in the remainder of this thesis.


\section{Difficulty}

	The difficulty of this problem can be seen by recent work that generalizes the results of Friedgut, Kalai, and Nisan. First, its difficulty can be seen by Friedgut, Kalai, and Nisan themselves failing to generalize it, both in the original paper \cite{friedgut2008elections}, and also later when they removed the neutrality constraint \cite{friedgut2011quantitative}. If it were an easy task, they would have done it from the outset.

	In addition, other authors have done work along the same lines, but still without coming up with a general result. In 2008 Xia and Conitzer were able to prove a similar theorem for any number of candidates, but instead of neutrality they assumed 5 other conditions for the voting rule \cite{xia2008sufficient}:

	\begin{itemize}
		\item Homogeneity
		\item Anonymity
		\item Non-imposition
		\item Canceling out
		\item Stability
	\end{itemize}

	These conditions are formally defined and explained in Chapter 4: Related Work, and also by Xia and Conitzer themselves.

	These conditions are stricter than the neutrality assumption of Friedgut, Kalai, and Nisan, in the sense that they do not capture all of the ``common'' voting rules, e.g. Bucklin.

	Around the same time Dobzinski and Procaccia published complementary results for two voters and social choice functions satisfying unanimity (the Pareto principle) \cite{dobzinski2008frequent}. They proved the following:

	\begin{theorem}[Dobzinski and Procaccia]
		Let $f$ be a Pareto-optimal SCF and let $n = 2$, $m \ge 3$, and $\delta < \frac{1}{32m^9}$. If $f$ is $\delta$-strategyproof then $f$ is $16m^8 \delta$-dictatorial.
	\end{theorem}

	The fact that all of these authors worked on the same problem over multiple years and were unable to achieve a general result speaks to its difficulty.


\section{Independent Work}

	Unfortunately for us, but fortunately for the field of social choice theory as a whole, Isaksson, Kindler, and Mossel \emph{have}, independently and during the writing of this thesis, published a brilliant generalization of the original theorem of Friedgut, Kalai, and Nisan and even improved slightly upon the results \cite{isaksson2010geometry}. Translating their results into the terminology we have been using, they proved that for a neutral social choice function $f$ with $m \ge 4$ alternatives and $n$ voters that is $\epsilon$-far from dictatorship, a uniformly chosen profile will be manipulable with probability at least $2^{-1} \epsilon^2 n^{-4} m^{-6} (m!)^{-3}$.

	Later Friedgut et al. removed the neutrality constraint from their original theorem, and added an author \cite{friedgut2011quantitative}.

	Finally, Mossel and R\'{a}cz \cite{mossel2011quantitative} took ideas from these two proofs and created a unified proof with the same results as Isaksson, Kindler, and Mossel, but without the neutrality constraint.

	Though these results have independently achieved the goals we set out with, we believe that our work is still useful. At the very least ours simply stands as an alternate proof. However, our proof has the benefit that it uses very similar techniques to those of the original proof by Friedgut, Kalai, and Nisan, and additionally we believe that our proof is much simpler and more easily understood.


\section {Proof Summary}

	Here we summarize our generalization and our main result, both for reference and to act as an outline of our method. Some of this notation will be defined in subsequent chapters, and might not be understood until then.

	The proof we are generalizing is broken up into three steps. In the original paper they are called Step 1, Step 2, and Step 3, and in \cite{friedgut2011quantitative} they are called the following respectively:
	\begin{description}
		\item[Step 1:] application of a quantitative version of Arrow's impossibility theorem,
		\item[Step 2:] reduction from an SCF with low dependence on irrelevant alternatives to a GSWF with a low paradox probability,
		\item[Step 3:] reduction from low manipulation power to low dependence on irrelevant alternatives,
	\end{description}

	In the original paper, Friedgut, Kalai, and Nisan were able to generalize Step 1 and Step 2 as follows:

	\begin{lemma}[Generalized Step 1]
		For every fixed $m$ and $\epsilon > 0$ there exists $\delta > 0$ such that if $F = f^{\otimes \binom{m}{2}}$ is a neutral IIA GSWF over $m$ alternatives with $f : \{0,1\}^n \rightarrow \{0,1\}$, and $\Delta(f, DICT) > \epsilon$, then $F$ has probability of at least $\delta \ge (C\epsilon)^{\lfloor m/3 \rfloor}$ of not having a Generalized Condorcet Winner, where $C > 0$ is an absolute constant.
	\end{lemma}

	\begin{lemma}[Generalized Step 2]
		For every fixed $m$ there exists $\delta > 0$ such that for all $\epsilon > 0$ the following holds. Let $f$ be a neutral SCF among $m$ alternatives such that $\Delta(f, DICT) > \epsilon$. Then for all $(a,b)$ we have $M^{a,b}(f) \ge \delta$.
	\end{lemma}

	Therefore, we focus on generalizing Step 3 so that together with the already generalized Steps 1 and 2 we will have a generalized main theorem. The original Step 3 was:

	\begin{lemma}[Non-General Step 3]
		For every SCF $f$ on $3$ alternatives and every $a,b \in A$, $M^{a,b} \le \sum_i M_i \cdot 6$
	\end{lemma}

	And the generalization we attempt to prove is:

	\begin{lemma}[Generalized Step 3]
		For every SCF $f$ on $m$ alternatives and every $a,b \in A$, $M^{a,b} \le \sum_i M_i \cdot m!$
	\end{lemma}

	When we put together all 3 generalized steps we get our main result:
	\begin{theorem}[Main Result]
		There exists a constant $C > 0$ such that for every $\epsilon > 0$ the following holds. If $f$ is a neutral SCF for $n$ voters over $m$ alternatives and $\Delta(f, g) > \epsilon$ for any dictatorship $g$, then $f$ has total manipulability: $\sum^n_{i=1} M_i(f) \ge \frac{(C\epsilon)^{\lfloor m/3 \rfloor}}{m!}$.
	\end{theorem}


\section{Structure of the Remaining Chapters}

	\begin{description}
		\item[Chapter 2: Preliminaries.] In the next chapter we introduce the preliminaries, which include formal definitions and notation to serve as a reference for use in the rest of the thesis. The preliminaries are often elementary and provide a technical foundation for the following work.

		\item[Chapter 3: Background.] Here we give some background information on the field of social choice theory and describe how it has evolved, leading to the problem we are addressing in this work.

		\item[Chapter 4: Related Work.] In the related work chapter we describe, in a moderate amount of detail, the results and methods of various other authors relating both to this thesis, and to the work of Friedgut, Kalai, and Nisan on which our work is based.

		\item[Chapter 5: Results.] This is the technical portion of the thesis in which we prove foundational lemmas and build up a proof of our main result.
	\end{description}
